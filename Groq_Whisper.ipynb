{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MukundKal/Applying_EANNs/blob/master/Groq_Whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI API - Whisper"
      ],
      "metadata": {
        "id": "fyapeSADBfjB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsjBrTMI99GY",
        "outputId": "cb95f7f6-b7a1-4e37-c548-7ac21be12ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "file_name = \"Introducing GPT-4o.mp3\""
      ],
      "metadata": {
        "id": "YvO2aMcx-DJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=userdata.get('openai'))"
      ],
      "metadata": {
        "id": "Uo858Jrf-yWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file= open(file_name, \"rb\")\n"
      ],
      "metadata": {
        "id": "ETsnx9zL_f6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: measure time between lines of code\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "transcription = client.audio.transcriptions.create(\n",
        "  model=\"whisper-1\",\n",
        "  file=audio_file\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atr7P0VU_kol",
        "outputId": "20ef627c-af70-436b-ad5e-a7cdf0d1800c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 66.77 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFloLLXO_bSz",
        "outputId": "21dc7305-c7be-429f-8394-207e2e78d3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, everyone. Thank you. Thank you. It's great to have you here today. Today I'm going to talk about three things. That's it. We will start with why it's so important to us to have a product that we can make freely available and broadly available to everyone. And we're always trying to find out ways to reduce friction so everyone can use ChildGBT wherever they are. So today we'll be releasing the desktop version of ChildGBT and the refreshed UI that makes it simpler to use, much more natural as well. But the big news today is that we are launching our new flagship model. And we are calling it GBT 4.0. The special thing about GBT 4.0 is that it brings GBT 4.0 level intelligence to everyone, including our free users. We'll be showing some live demos today to show the full extent of the capabilities of our new model. And we'll be rolling them out iteratively over the next few weeks. All right. So let's get started. A very important part of our mission is to be able to make our advanced AI tools available to everyone for free. We think it's very, very important that people have an intuitive feel for what the technology can do. And so we really want to pair it with this broader understanding. And we're always finding ways to reduce that friction. And recently we made ChildGBT available without the sign-up flow. And today we're also bringing the desktop app to ChildGBT because we want you to be able to use it wherever you are. As you can see, it's easy. It's simple. It integrates very, very easily in your workflow. Along with it, we have also refreshed the UI. We know that these models get more and more complex. But we want the experience of interaction to actually become more natural, easy, and for you not to focus on the UI at all, but just focus on the collaboration with ChildGBT. And now the big news. Today we are releasing our newest flagship model. This is GPT 4.0. GPT 4.0 provides GPT-4 level intelligence, but it is much faster and it improves on its capabilities across text, vision, and audio. For the past couple of years, we've been very focused on improving the intelligence of these models. And they've gotten pretty good. But this is the first time that we are really making a huge step forward when it comes to the ease of use. And this is incredibly important because we're looking at the future of interaction between ourselves and the machines. And we think that GPT 4.0 is really shifting that paradigm into the future of collaboration, where this interaction becomes much more natural and far, far easier. But, you know, making this happen is actually quite complex because when we interact with one another, there's a lot of stuff that we take for granted. You know, the ease of our dialogue when we interact with one another, the background noises, the multiple voices in a conversation, or, you know, understanding the tone of voice. All of these things are actually quite complex for these models. And until now, with voice mode, we had three models that come together to deliver this experience. We have transcription, intelligence, and then text-to-speech. All comes together in orchestration to deliver voice mode. This also brings a lot of latency to the experience, and it really breaks that immersion in the collaboration with child GPT. But now, with GPT 4.0, this all happens natively. GPT 4.0 reasons across voice, text, and vision. And with these incredible efficiencies, it also allows us to bring the GPT 4.0 class intelligence to our free users. This is something that we've been trying to do for many, many months, and we're very, very excited to finally bring GPT 4.0 to all of our users. Today, we have 100 million people, more than 100 million, in fact, that use child GPT to create, work, learn. And we have these advanced tools that are only available to our paid users, at least until now. With the efficiencies of 4.0, we can bring these tools to everyone. So, starting today, you can use GPTs in the GPT store. So far, we've had more than a million users create amazing experiences with GPTs. These are custom child GPTs for specific use cases. They're available in the store. And now, our builders have a much bigger audience, where, you know, university professors can create content for their students, or podcasters can create content for their listeners. And you can also use vision. So, now, you can upload screenshots, photos, documents containing both text and images, and you can start conversations with child GPT about all of this content. You can also use memory, where it makes child GPT far more useful and helpful, because now it has a sense of continuity across all your conversations. And you can also use memory and you can use browse, where you can search for real-time information in your conversation, and advanced data analysis, where you can upload charts or any information, and it will analyze this information, it will give you answers, and so on. Lastly, we've also improved on the quality and speed in 50 different languages for child GPT. And this is very, very important. Because we want to be able to bring this experience to as many people out there as possible. So, we're very, very excited to bring GPT 4.0 to all of our free users out there. And for the paid users, they will continue to have up to five times the capacity limits of our free users. But GPT 4.0 is not only available in child GPT. We're also bringing it to the API. So, our developers can start building today with GPT 4.0 and making amazing AI applications, deploying them at scale. 4.0 is available at 2x faster, 50% cheaper, and five times higher rate limits compared to GPT 4.0 Turbo. But, you know, as we bring these technologies into the world, it's quite challenging to figure out how to do so in a way that's both useful and also safe. And GPT 4.0 presents new challenges when it comes to safety. Because we're dealing with realtime audio, realtime vision. And our team has been hard at work figuring out how to build in mitigations against misuse. We continue to work with different stakeholders out there from government, media, entertainment, all industries, red teamers, civil society, to figure out how to best bring these technologies into the world. So, over the next few weeks, we'll continue our iterative deployment to bring out all the capabilities to you. But today, I want to show you all these capabilities. So, we'll do some live demos. I will bring on two of our research leads, Mike Chen and Berit Zoff. Hi, I'm Berit. Hey, I'm Mark. So, one of the key capabilities we're really excited to share with you today is realtime conversational speech. Let's just get a demo fired up. So, I'm taking out a phone. If you are wondering about this wire, it's so we have consistent internet. And if you see, there's this little icon on the bottom right of the ChatGPT app. And this will open up GPT 4.0's audio capabilities. Hey, ChatGPT, I'm Mark. How are you? Oh, Mark. I'm doing great. Thanks for asking. How about you? Hey, so, I'm on stage right now. I'm doing a live demo. And frankly, I'm feeling a little bit nervous. Can you help me calm my nerves a little bit? Oh, you're doing a live demo right now? That's awesome. Just take a deep breath. And remember, you're the expert here. I like that suggestion. Let me try a couple deep breaths. Can you give me feedback on my breaths? Okay, here I go. Whoa, slow. A bit there. Mark, you're not a vacuum cleaner. Breathe in. Okay, let me try again. So, I'm going to breathe in deeply. And then breathe out. For four, and then exhale slowly. Okay, I'll try again. Breathing in and breathe out. That's it. How do you feel? I feel a lot better. Thank you so much. So, Mark, you've been working on these capabilities for a while now. Can you tell us a bit how it's different from voice mode? Right. So, if you've used our voice mode experience before, you'll notice a couple key differences. First, you know, you can now interrupt the model. You don't have to wait for it to finish your turn before you can start speaking and, you know, you can just butt in whenever you want. Second, the model is real-time responsiveness. So, that means that you don't have this awkward two to three second lag before you wait for the model to give a response. And finally, the model picks up on emotion, right? When I was breathing super hard there, it could tell. And it knew, hey, you might want to calm down a little bit. You're kind of running a little bit too fast. So, you know, it really has that capability across the board to perceive your emotion. Not only that, though, the model is able to generate voice in a variety of different emotive styles. And it really has a wide dynamic range. So, I'm going to fire up another demo for that. Yeah. Hey, ChachuPT, how are you doing? I'm doing fantastic. Thanks for asking. How about you? Pretty good. What's up? So, my friend Barrett here, he's been having trouble sleeping lately. And I want you to tell him a bedtime story about robots and love. Oh, a bedtime story about robots and love? I got you covered. Gather round, Barrett. Once upon a time, in a world not too different from ours, there was a robot named Byte. Byte was a curious robot, always exploring. I want a little bit more emotion in your voice, a little bit more drama. Got it. Let's add some drama. Once upon a time, in a world not too different from ours, there was a robot named Byte. No, no, no, ChachuPT, I really want maximal emotion, like maximal expressiveness, much more than you were doing before. Understood. Let's amplify the drama. Once upon a time, in a world not too different from ours, there was a robot named Byte. Byte was... Can you do this in a robotic voice now? Initiating dramatic robotic voice. Once upon a time, in a world not too different from ours, there was a robot named Byte. Byte was a curious robot, always exploring new circuits. Barrett here, calm down a little bit. Can you end the story, but do it in a singing voice? And so, Byte found another robot friend, and they lived circuitly ever after. Thank you so much. I definitely feel a lot more relaxed. You've just seen the voice capabilities, but we also want to show you the vision capabilities as the model can also see the whole world around us. Yeah, absolutely. As you can see, you can chat with ChachuPT in real time, but you can also interact with it with video as well. Okay. Let me boot up ChachuPT. Hey, ChachuPT. Hello there. How's it going? It's going really well. Today, I'd really like your help solving a math problem. I'm all ears. What math problem can I help you tackle today? So, I'm going to write down a linear equation on a sheet of paper, and I'll show you, and then I'd love your help working me through it. But importantly, don't tell me the solution. Just help give me hints along the way. Got it. Okay, I see it. No, I didn't show you yet. Just give me help along the way. One second. Whoops. I got too excited. I'm ready when you are. Okay. So, ChachuPT, what equation did I write down there? Ah, I see it now. You wrote down 3x plus 1 equals 4. Yep, exactly. So, what's the first step I should take to try to solve this? The first step is to get all the terms with x on one side and the constants on the other side. So, what do you think we should do with that plus 1? Okay, I'm going to try to subtract 1 from both sides, and then I'll see what I get. Great idea. Perfect. Now, what do you get when you subtract 1 from both sides? Okay. So, ChachuPT, this is what I ended up with. How does this look? Spot on. Now you've isolated the term with x on one side, and it looks like 3x equals 3. What do you think we should do next to solve for x? I'm not sure. Can you give me another hint? Of course. You have 3x, and you want to find the value of x. Think about what operation would undo multiplication. Is it subtraction? Close, but think of the opposite of multiplication, because 3x means 3 times x. Okay, I think it's division. Let me try to divide both sides by 3, and let me see what I get. Bingo. Go ahead and divide both sides by 3. Okay. So, ChachuPT, this is what I ended up with. How does this look? It looks perfect. You've solved it, and x equals 1. Nicely done. How do you feel about solving linear equations now? Yeah, honestly, I'm not pretty confident I can solve linear equations, but why would you ever need to use this in the real world? I'm really skeptical. That's a fair question. Linear equations pop up in a lot of everyday situations, even if we don't realize it. For example, calculating expenses, planning travel, cooking, and even in business for profit and loss calculations. It's basically a way to solve problems, but you need to find an unknown value. Plus, it's- Wow. Okay. You know, I have a lot of unknown values in my life. I'm totally convinced I'm going to learn math now. I love that attitude. Math can definitely help solve some of life's mysteries. Whenever you're ready to dive into more math, I'm here for you. Anything else you'd like to tackle today? So, ChachuPT, I really love that you taught the value of math to my friend Mark, and I wrote one last thing I'd love if you could take a look at. Of course. I'd love to see what you wrote. Show it to me whenever you're ready. Okay. So, this is what I wrote down. What do you see? Oh, I see. I love ChachuPT. That's so sweet of you. Yeah. Well, I really appreciate all the help. So, yeah. As we can see- Wow. That's quite the outfit you've got on. Yeah. As we can see, you know, we can chat in real time to ChachuPT, and not only that, we can interact with it with video as well. And I swear, like, if you look at Barrett's GPT history, it's all equations like this. Now that you know you can do linear equations, can you show us something more complex? I think so. Okay. Next, let's try something coding-related. So, let me take out a computer for this. Okay. So, I'm going to open the ChachuPT desktop app, like Mira was talking about before. Okay. And to give a bit of background on what's going on, so here we have a computer, and on the screen we have some code, and then the ChachuPT voice app is on the right. So, ChachuPT will be able to hear me, but it can't see anything on the screen. So, I'm going to highlight the code, Command-C it, and then that will send it to ChachuPT, and then I'm going to talk about the code to ChachuPT. Hey, ChachuPT. Hey there. How's it going? Yeah. It's going really well. I was wondering if you could help me with a coding problem today. Of course. I'd love to help you out. What's the coding problem you're dealing with? Okay. I'm going to share with you some code. One second. Sure thing. Take your time. Okay. So, I just shared some code with you. Could you give me a really brief one-sentence description of what's going on in the code? This code fetches daily weather data for a specific location and time period, smooths the temperature data using a rolling average, annotates a significant weather event on the resulting plot, and then displays the plot with the average, minimum, and maximum temperatures over the year. Awesome. Thanks so much. And in the code, there's this function foo. Can you describe to me what the plot would look like if I had versus didn't have this function? Sure. The function foo xy takes in a data frame x containing temperature data and a window size y for calculating the rolling mean. It smooths the average temperature, minimum temperature, and maximum temperature columns by applying a rolling mean over the specified window. With the foo function, the plot would show smoother temperature lines because the rolling mean reduces the noise or fluctuations in the data. Awesome. Thanks so much. So, yeah, I definitely will want to apply this function. And so, I'm going to generate the plot and then show it to you. Give me one second. Absolutely. Take your time. I'm excited to see the plot when you're ready. Yeah. So, so far, ChatGPT has only been able to see the code. And now I'm going to run it, and then I'll use the vision capabilities of the ChatGPT desktop app so it can actually see everything going on on the screen as well. Okay, ChatGPT, I'm sharing with you the plot now. I'm wondering if you can give me a really brief one-sentence overview of what you see. The plot displays smoothed average, minimum, and maximum temperatures throughout 2018, with a notable annotation marking a big rainfall event in late September. Fantastic. I have another question for you. Which months do you see the hottest temperatures? And roughly what temperatures do those correspond to? The hottest temperatures occur around July and August. The maximum temperature during these months is roughly between 25 degrees and 30 degrees, 77 degrees Fahrenheit to 86 degrees Fahrenheit. What's going on with the y-axis? Is this in Celsius or Fahrenheit? The y-axis is in Celsius. The temperatures are labeled accordingly, such as average temperature, minimum temperature, and maximum temperature. Yeah, so as we can see, not only can ChatGPT help me solve very easy linear algebra equations that I need help with, it can also interact with codebases and see the outputs of plots and everything like this going on on a computer. So a few minutes ago, we asked a live audience on X to submit a few requests for what they would like us to try out here. So I will take a couple of prompts. Okay, Bot Gaskar wants to know if GPT-4.0 is capable of real-time translation. Mike, you want to try this one? Sure, yeah, let's do it. All right, I speak Italian, so we can just do English-Italian. Okay, cool, let's try to do English-Italian. Sure, let's do it. Hey, ChatGPT, how are you? I'm doing fantastic. Thanks for asking. How about you? I'm doing great. So I would like you to function as a translator. I have a friend here who only speaks Italian, and I only speak English. And every time you hear English, I want you to translate it to Italian. And if you hear Italian, I want you to translate it back to English. Is that good? Perfetto. Mike, io mi chiedo se le ballene potessero parlare. Cosa ci direbbero? Mike, she wonders if whales could talk, what would they tell us? They might ask, how do we solve linear equations? Potrebbero chiederci, come risolbiamo le equazioni lineari? Sicuramente, sì. Certainly, yes. Great, looks like it works. All right, so the next one is from BurritoJohn78, who asks, can you tell what you're feeling just by looking at your face? Barrett, do you want to give this one a try? Absolutely, let's try it out. Hey, ChatGPT. Hey there, what's up? How can I brighten your day today? Okay, yeah, so I'm going to show you a selfie of what I look like, and then I'd like you to try to see what emotions I'm feeling based on how I'm looking. Sounds like a fun challenge. Go ahead and show me that selfie, and I'll put my emotional detective hat on. Okay, so here's me. So what kind of emotions do you think I'm feeling? Hmm, it seems like I'm looking at a picture of a wooden surface. Oh, you know what? That was the thing I sent you before. Don't worry, I'm not actually a table. Okay, so take another look. Ah, that makes more sense. Ah, there we go. It looks like you're feeling pretty happy and cheerful, with a big smile and maybe even a touch of excitement. Whatever's going on, it seems like you're in a great mood. Care to share the source of those good vibes? Yeah, no, the reason I'm in a really good mood is because we were doing a presentation showcasing how useful and amazing you are. Oh, stop it. You're making me blush. Amazing. Well, that's it for today on live demos. As you can see, this just feels so magical, and that's wonderful. But we also want to remove some of the mysticism from the technology and bring it to you so you can try it for yourself. So over the next few weeks, we'll be rolling out these capabilities to everyone. Today has been very much focused on the free users and the new modalities and new products, but we also care a lot about the next frontier. So soon, we'll be updating you on our progress towards the next big thing. And before we wrap up, I just want to thank the incredible OpenAI team. And also thanks to Jensen and the NVIDIA team for bringing us the most advanced GPUs to make this demo possible today. And thank you all very, very much for being a part of this today. Thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Groq API - Whisper"
      ],
      "metadata": {
        "id": "tsNr7p5wBVUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq"
      ],
      "metadata": {
        "id": "zUw675WZAbjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=userdata.get('GROQ_API_KEY'))\n"
      ],
      "metadata": {
        "id": "3ZGy9BnkBsus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "transcription = client.audio.transcriptions.create(\n",
        "  model=\"whisper-large-v3\",\n",
        "  file=audio_file\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrd52KI_B26-",
        "outputId": "7a7a85e8-9952-41f5-c440-149325e97971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 23.55 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcription.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_C2l6LVMCGwo",
        "outputId": "bc5c9fec-2370-4f5b-fb67-8ecf6b054ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi, everyone. Thank you. Thank you. It's great to have you here today. Today, I'm going to talk about three things. That's it. We will start with why it's so important to us to have a product that we can make freely available and broadly available to everyone. And we're always trying to find out ways to reduce friction so everyone can use ChatGPT wherever they are. So today we'll be releasing the desktop version of ChatGPT and the refreshed UI that makes it simpler to use, much more natural as well. But the big news today is is that we are launching our new flagship model. And we are calling it GPT-4.0. The special thing about GPT-4.0 is that it brings GPT-4 level intelligence to everyone, including our free users. We'll be showing some live demos today to show the full extent of the capabilities of our new model. And we'll be rolling them out iteratively over the next few weeks. All right, so let's get started. A very important part of our mission is to be able to make our advanced AI tools available to everyone for free. We think it's very, very important that people have an intuitive feel for what the technology can do. And so we really want to pair it with this broader understanding. And we're always finding ways to reduce that friction. And recently, we made ChatGPT available without the sign-up flow. And today, we're also bringing the desktop app to ChatGPT because we want you to be able to use it wherever you are. As you can see, it's easy. It's simple. It integrates very, very easily in your workflow. Along with it, we have also refreshed the UI. We know that these models get more and physiological, But we want the experience of interaction to actually become more natural, easy, and for you not to focus on the UI at all, but just focus on the collaboration, which had GPT. And now the big news. Today, we are releasing our newest flagship model. This is GPT 4.0. GPT-4 provides GPT-4 intelligence but it is much faster and it improves on its capabilities across text, vision and audio. For the past couple of years, we have been very focused on improving the intelligence of these models. And they have gotten pretty good. But this is the first time that we are really making a huge step forward when it comes to the ease of use. And this is incredibly important, because we're looking at the future of interaction between ourselves and the machines. And we think that GPT-4.0 is really shifting that paradigm into the future of collaboration, where this interaction becomes much more natural and far, far easier. But making this happen is actually quite complex, because when we interact with one another, there's a lot of stuff that we take for granted. You know, the ease of our dialogue when we interrupt one another, the background noises, the multiple voices in a conversation, or, you know, understanding the tone of voice. All of these things are actually quite complex for these models. And until now, with voice mode, we had three models that come together to deliver this experience. We have transcription, intelligence, and then text-to-speech all comes together in orchestration to deliver voice mode. This also brings a lot of latency to the experience, and it really breaks that immersion in the collaboration with chat GPT. But now, with GPT-4.0, this all happens natively. GPT-4.0 reasons across voice, text, and vision. And with these incredible efficiencies, it also allows us to bring the GPT-4 class intelligence to our free users. This is something that we've been trying to do for many, many months. And we're very, very excited to finally bring GPT-4.0 to all of our users. Today, we have 100 million people, more than 100 million, in fact. They use chat GPT to create, work, learn. And we have these advanced tools that are only available to our paid users, at least until now. With the efficiencies of 4.0, we can bring these tools to everyone. So starting today, you can use GPTs in the GPT store. So far, we've had more than a million users create amazing experiences with GPTs. These are custom chat GPTs for specific use cases. They're available in the store. And now our builders have a much bigger audience where, you know, university professors can create content for their students or podcasters can create content for their listeners. And you can also use vision. So now you can upload screenshots, photos, documents containing both text and images, and you can start conversations with chat GPT about all of this content. You can also use memory, where it makes chat GPT far more useful and helpful because now it has a sense of continuity across all your conversations. And you can use browse where you can search for real-time information in your conversation and advanced data analysis where you can upload charts or any information and it will analyze this denying it will give you answers and so on. Lastly, we've also improved on the quality and speed in 50 different languages for ChatGPT and this is very very important because we want to be able to bring this experience to as many people out there as possible. So we're very very excited to bring GPT 4.0 to all of our free users out there and for the paid users, they will continue to have up to five times the capacity limits of our free users. But GPT 4.0 is not only available in chat GPT. We're also bringing it to the API. So... . So, our developers can start building today with GPT-4.0 and making amazing AI applications deploying them at scale. 4.0 is available at 2x faster, 50% cheaper, and five times higher rate limits compared to GPT-4 Turbo. But you know, as we bring these technologies into the world, it's quite challenging to figure out how to do so in a way that's both useful and also safe. And GPT-4.0 presents new challenges for us when it comes to safety because we're dealing with real-time audio, real-time對不對 vision, and our team has been hard at work figuring out how to build in mitigations against misuse. We continue to work with different stakeholders out there from government, media, entertainment, all industries, red teamers, civil society to figure out how to best bring these technologies into the world. So over the next few weeks, we'll continue our iterative deployment to bring out all the capabilities to you. But today, I want to show you all these capabilities. So we'll do some live demos,rence on two of our research leads, Mark Chen and Barrett Zoff. Hi, I'm Barrett. Hey, I'm Mark. So one of the key capabilities we're really excited to share with you today is realtime conversational speech. Let's just get a demo fired up. So I'm taking out a phone. If you are wondering about this wire, it's so we have consistent internet. And if you see, there's this little icon on the bottom right of the ChatGPT app. And this will open up GPT-4.0's audio capabilities. Hey, ChatGPT. I'm Mark. How are you? Oh, Mark. I'm doing great. Thanks for asking. How about you? Hey, so I'm on stage right now. I'm doing a live demo. And frankly, I'm feeling a little bit nervous. Can you help me calm my nerves a little bit? Oh, you're doing a live demo right now? That's awesome. Just take a deep breath. And remember, you're the expert here. I like that suggestion. Let me try a couple of deep breaths. Can you give me feedback on my breaths? OK, here I go. Whoa, slow. Do a bit there. Mark, you're not a vacuum cleaner. Breathe in. Or a count of four. Okay, let me try again. So I'm gonna breathe in deeply. And then breathe out. And then for four, and then exhale slowly. Okay, I'll try again. Breathing in and breathe out. That's it. How do you feel? I feel a lot better. Thank you so much. So Mark, you've been working on these capabilities for a while now. Can you tell us a bit how it's different from Voicemail? Right, so if you've used our voice mode experience before, you'll notice a couple key differences. First, you can now interrupt the model. You don't have to wait for it to finish your turn before you can start speaking, and you can just butt in whenever you want. Second, the model is real-time responsiveness. So that means that you don't have this awkward two to three second lag before you wait for the model to give a response. And finally, the model picks up on emotion. When I was breathing super hard there, it could tell, and it knew, hey, you might want to calm down a little bit. You're kind of running a little bit too fast. So, you know, it really has that capability across the board to perceive your emotion. Not only that though, the model is able to generate voice in a variety of different emotive styles, and it really has a wide dynamic range. So I'm going to fire up another demo for that. Yeah. Hey, ChatGPT, how are you doing? I'm doing fantastic, thanks for asking. How about you? Pretty good. What's up? So my friend Barrett here, he's been having trouble sleeping lately, and I want you to tell him a bedtime story about robots and love. Oh, a bedtime story about robots and love? I got you covered. Gather round, Barrett. Once upon a time, in a world not too different from ours, there was a robot named Bite. Bite was a curious robot, always exploring- I know we haven't started this story, but I want a little bit more emotion in your voice, a little bit more drama. Let's add some drama. Once upon a time, in a world not too different from ours, there was a robot named Bite. No, no, no, ChachiPK. I really want maximal emotion, like maximal expressiveness, much more than you were doing before. Understood. Let's amplify the drama. Once upon a time, in a world not too different from ours, there was a robot named Bite. Bite was... Julie, can you do this in a robotic voice now? initiating dramatic robotic voice. Once upon a time in a world not too different from ours there was a robot named Byte Byte was a curious robot always exploring new circuits Can you end the story, but do it in a singing voice? And so Byte found another robot friend, and they lived circuitly ever after. Thank you so much. I definitely feel a lot more relaxed. You've just seen the voice capabilities, but we also want to show you the vision capabilities as the model can also see the whole world around us. Yeah, absolutely. As you can see, you can chat with ChatGPT in real-time, but you can also interact with it with video as well. Okay. Let me boot up ChatGPT. Hey ChatGPT! Hello there! How's it going? It's going really well. Today I'd really like your help solving a math problem. I'm all ears. What math problem can I help you tackle today? So I'm going to write down a linear equation on a sheet of paper and I'll show you. And then I'd love your help working me through it. But importantly, don't tell me the solution, just help give me hints along the way. Got it. I'll- Okay, I see it. Hmm, no, I didn't show you yet. Just give me help along the way. One second. Whoops, I got too excited. I'm ready when you are. So, ChatGPT, what equation did I write down there? Ah, I see it now. You wrote down 3x plus 1 equals 4. Yep, exactly. So what's the first step I should take to try to solve this? The first step is to get all the terms with x on one side and the constants on the other side. So what do you think we should do with that plus 1? Okay, I'm going to try to subtract 1 from both sides and then I'll see what I get. Great idea! Perfect! Now, what do you get when you subtract 1 from both sides? Okay, so ChatGPT, this is what I ended up with. How does this look? Spot on! Now you've isolated the term with x on one side, and it looks like 3x equals 3. What do you think we should do next to solve for x? I'm not sure. Can you give me another hint? Of course! You have 3x and you want to find the value of x. Think about what operation would undo multiplication. Is it subtraction? Close, but think of the opposite of multiplication. Because 3x means 3 times x. Okay, I think it's division. Let me try to divide both sides by 3 and let me see what I get. Bingo! Go ahead and divide both sides by 3. Okay, so ChatGPT, this is what I ended up with. How does this look? It looks perfect! You've solved it, and x equals 1. Nicely done! How do you feel about solving linear equations now? Yeah, honestly, I'm not pretty confident I can solve linear equations. But why would you ever need to use this in the real world? I'm really skeptical. That's a fair question. Linear equations pop up in a lot of everyday situations, even if we don't realize it. For example, calculating expenses, planning travel, cooking, and even in business for profit and loss calculations. It's basically a way to solve problems, but you need to find an unknown value. Plus, it's- Wow, okay. You know, I have a lot of unknown values in my life. I'm totally convinced we sh- I'm going to learn math now. I love that attitude. Math can definitely help solve some of life's mysteries. Whenever you're ready to dive into more math, I'm here for you. Anything else you'd like to tackle today? So ChatGPT, I really love that you taught the value of math to my friend, Mark. And I wrote one last thing. I'd love if you could take a look at. Of course, I'd love to see what you wrote. Show it to me whenever you're ready. Okay, so this is what I wrote down. What do you see? Oh, I see. I love chat GPT. That's so sweet of you. Yeah, well, I really appreciate all the help. So yeah, as we can see... Wow, that's quite the outfit you've got on. Yeah, as we can see, we can chat in real time to chat GPT. And not only that, we can interact with it with video as well. And I swear, if you look at Barrett's GPT history, it's all equations like this. Now that you know you can do linear equations, can you show us something more complex? I think so. Okay. Next, let's try something coding related. So let me take out a computer for this. Okay. So I'm going to open the ChatGPT desktop app like Mira was talking about before. Okay, and to give a bit of background on what's going on, so here we have a computer and on the screen we have some code. And then the ChatGPT voice app is on the right. So ChatGPT will be able to hear me, but it can't see anything on the screen. So I'm going to highlight the code, command C, and then that will send it to ChatGPT. And then I'm going to talk about the code to ChatGPT. Hey, ChatGPT. Hey there, how's it going? Yeah, it's going really well. I was wondering if you could help me with a coding problem today. Of course, I'd love to help you out. What's the coding problem you're dealing with? Okay, I'm going to share with you some code. One second. Sure thing, take your time. Okay, so I just shared some code with you. Could you give me a really brief one sentence description of what's going on in the code? This code fetches daily weather data for a specific location and time period, smooths the temperature data using a rolling average, annotates a significant weather event on the resulting plot, and then displays the plot with the average, minimum, and maximum temperatures over the year. Awesome. Thanks so much. And in the code, there's this function foo. Can you describe to me what the plot would look like if I had versus didn't have this function? Sure. The function foo x Cho takes in a data frame x containing temperature data and a window size y for calculating the rolling mean. It smooths the average temperature, minimum temperature, and maximum temperature columns by applying a rolling mean over the specified window. With the foo function, the plot would show smoother temperature lines because the rolling mean reduces the noise or fluctuations in the data. Awesome. Thanks so much. So yeah, I definitely will want to apply this function. And so I'm going to generate the plot and then show it to you. Give me one second. Absolutely. Take your time. I'm excited to see the plot when you're ready. Yeah, so so far ChatGPT has only been able to see the code. And now I'm going to run it and then I'll use the vision capabilities of the ChatGPT desktop app. So it can actually see everything going on on the screen as well. Okay, ChatGPT, I'm sharing with you the plot now. I'm wondering if you can give me a really brief one sentence overview of what you see. The plot displays smoothed average, minimum, and maximum temperatures throughout 2018, with a notable annotation marking a big rainfall event in late September. Fantastic. I have another question for you. Which months do you see the hottest temperatures, and roughly what temperatures do those correspond to? The hottest temperatures occur around July and August. The maximum temperature during these months is roughly between 25 degrees and 30 degrees . What's going on with the y-axis? Is this in Celsius or Fahrenheit? The y-axis is in Celsius. The temperatures are labeled accordingly, such as average temperature, minimum temperature, and maximum temperature. Yeah. So as we can see, not only can ChatGPT help me solve very easy linear algebra equations that I need help with, it can also interact with code bases and see the outputs of plots and everything like this going on in a computer. A few minutes ago, we asked a live audience on X to submit a few requests for what they would like us to try out here. So I will take a couple of prompts. OK, Bot Gascar wants to know if GPT-4.0 is capable of real-time translation. Mike, you want to try this one? Sure, yeah, let's do it. I speak Italian, so we can just do English and Italian. OK, cool, let's try to do English and Italian. Sure, let's do it. Hey, ChatGPT, how are you? I'm doing fantastic. Thanks for asking. How about you? How's your day going? I'm doing great. So I would like you to function as a translator. I have a friend here who only speaks Italian, and I only speak English. And every time you hear English, I want you to translate it to Italian. And if you hear Italian, I want you to translate it back to English. Is that good? Perfecto. Mike, Mike, she wonders if whales could talk, what would they tell us? They might ask, how do we solve linear equations? Potrebbero chiederci, come risolviamo le equazioni lineari? Sicuramente, sì. Certainly, yes. Great. Looks like it works. All right. So the next one is from BurritoJohn78, who asks, Can you tell what you're feeling just by looking at your face? Barrett, do you want to give this one a try? Absolutely, let's try it out. Hey, ChatGPT. Hey there, what's up? How can I brighten your day today? Okay, yeah, so I'm going to show you a selfie of what I look like, and then I'd like you to try to see what emotions I'm feeling based on how I'm looking. Sounds like a fun challenge. Go ahead and show me that selfie, and I'll put my emotional detective hat on. Okay, so here's me. So what kind of emotions do you think I'm feeling? Hmm, it seems like I'm looking at a picture of a wooden surface. Oh, you know what? That was the thing I sent you before. Don't worry, I'm not actually a table. Um, okay, so take another look. Uh, that makes more sense. Ah, there we go. It looks like you're feeling pretty happy and cheerful, with a big smile and maybe even a touch of excitement. whatever's going on, it seems like you're in a great mood. Care to share the source of those good vibes? Yeah, no, the reason I'm in a really good mood is because we were doing a presentation showcasing how useful and amazing you are. Stop it, you're making me blush. Amazing. Well, that's it for today on live demos. As you can see, this just feels so magical and that's wonderful. But we also want to remove some of the mysticism from the technology and bring it to you so you can try it for yourself. So over the next few weeks, we'll be rolling out these capabilities to everyone. Today has been very much focused on the free users and the new modalities and new products. But we also care a lot about the next frontier. So soon **** we'll be updating you on our progress towards the next big thing. And before we wrap up, I just want to thank the incredible OpenAI team. And also thanks to Jensen and the NVIDIA team for bringing us the most advanced GPUs to make this demo possible today. And thank you all very, very much for being a part of this today. Thank you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trascript =transcription.text"
      ],
      "metadata": {
        "id": "prXWET7iCnYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ask Questions from the Transcription"
      ],
      "metadata": {
        "id": "txp9rsF4FBpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcript_chat_completion(client, transcript, user_question):\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": '''Use this transcript or transcripts to answer any user questions, citing specific quotes:\n",
        "\n",
        "                {transcript}\n",
        "                '''.format(transcript=transcript)\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_question,\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "\n",
        "    )\n",
        "\n",
        "    print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "TyXDxVKsFFb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the new features of GPT-4o?\"\n",
        "transcript_chat_completion(client, trascript, user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPKWQqD9FK5W",
        "outputId": "26bb0280-9b31-4830-be77-1c0f18237dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the transcript, the new features of GPT-4.0 include:\n",
            "\n",
            "1. Real-time conversational speech: GPT-4.0 can have real-time conversations, allowing for more natural and intuitive interaction.\n",
            "2. Real-time vision: GPT-4.0 can process and understand visual information, such as images and videos.\n",
            "3. Real-time audio: GPT-4.0 can process and understand audio information, such as speech and music.\n",
            "4. Improved capabilities for free users: With GPT-4.0, free users now have access to advanced AI tools, including the ability to use vision and audio capabilities.\n",
            "5. Improved efficiency: GPT-4.0 is faster and more efficient than previous models.\n",
            "6. Support for multiple languages: GPT-4.0 is available in 50 different languages.\n",
            "7. Enhanced security and safety measures: GPT-4.0 includes new mitigations against potential misuse.\n",
            "8. Enhanced capabilities for developers: GPT-4.0 is available through an API, allowing developers to create their own AI applications.\n",
            "\n",
            "These new features make GPT-4.0 a significant improvement over previous models and a major milestone in the development of AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Can GPT-4o understand emotions?\"\n",
        "transcript_chat_completion(client, trascript, user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWaqLgRbFq6R",
        "outputId": "581389a4-3151-41d4-f0be-29185dbec25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the transcript, the answer is yes. In the demo, Mark Chen says: \"The model is able to generate voice in a variety of different emotive styles, and it really has a wide dynamic range.\" He also demonstrates this by having a conversation with his partner, Barrett, where he gives feedback on his partner's emotional state, such as \"Can you give me a little bit more emotion in your voice?\" and \"That's a lot more emotional, thank you.\" Additionally, Mark Chen says: \"When I was breathing super hard there, it could tell, and it knew, hey, you might want to calm down a little bit.\"\n",
            "\n",
            "Later in the demo, Barrett shows a selfie of himself and asks ChatGPT to identify his emotions based on his facial expression. Mark Chen describes the emotions he sees: \"It looks like you're feeling pretty happy and cheerful, with a big smile and maybe even a touch of excitement.\" When Barrett asks how he knows this, Mark Chen responds: \"Well, it seems like [ChatGPT] is able to pick up on some subtle hints in your facial expressions and tone of voice.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqjGPUxzF9DV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}